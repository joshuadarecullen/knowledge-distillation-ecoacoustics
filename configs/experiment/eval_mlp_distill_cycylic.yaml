
# @package _global_
# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: equal_split.yaml
  - override /model: MLPVAEModule.yaml
  - override /callbacks: mlp.yaml
  - override /trainer: gpu.yaml
  - override /logger: wandb.yaml # set logger here or use command line (e.g. `python train.py logger=tensorboard`)
  - override /paths: default.yaml
  - override /extras: default.yaml
  - override /hydra: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# task name, determines output directory path
task_name: "multitask_mlp_vae_distilled"

tags: ["dev", "Sussex Dataset", "MLP", "VAE", "student"]

# evaluate on test set, using best model weights achieved during training
# lightning chooses best weights based on the metric specified in checkpoint callback
test: True

# compile model for faster training with pytorch 2.0
compile: False

# simply provide checkpoint path to resume training
ckpt_path: '${paths.log_dir}multitask_mlp_vae_distilled/runs/2023-08-16_10-01-05/checkpoints/epoch_094.ckpt'

# seed for random number generators in pytorch, numpy and python.random
seed: 12345

data:
  seed: 12345

model:
  # chkpt: '${paths.log_dir}multitask_distill/runs/2023-08-15_20-53-55/checkpoints/epoch_043.ckpt'
  embedder: 'Eff'

trainer:
  min_epochs: 1
  max_epochs: 200
  # gradient_clip_val: 0.5
  fast_dev_run: False

logger:
  wandb:
    tags: ${tags}
    group: "MultiTask MLP EFFICIENTNET VAE"
  aim:
    experiment: "ECOACOUSTICS MultiTask MLP EFF VAE PREDICT"
