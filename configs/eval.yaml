# @package _global_

defaults:
  - _self_
  - model: MLPVAEModule.yaml
  - callbacks: mlp.yaml
  - logger: wandb.yaml
  - trainer: gpu.yaml
  - paths: default.yaml
  - extras: default.yaml
  - hydra: default.yaml
  - data: equal_split.yaml

task_name: "eval"

tags: ["dev"]

# passing checkpoint path is necessary for evaluation
# effnet
# ckpt_path: ${paths.log_dir}multitask_mlp_vae_distilled/runs/2023-08-28_18-11-33_89023/checkpoints/epoch_098.ckpt
# vae
# ckpt_path: ${paths.log_dir}multitask_mlp_vae_large/runs/2023-08-28_19-31-41_89023/checkpoints/epoch_075.ckpt
# vggish
# ckpt_path: ${paths.log_dir}train_mlpvggish/runs/2023-08-28_14-30-15_89023/checkpoints/last.ckpt
# effnet mlp

seed: 89023
data: 
   seed: ${seed}

# given how i coded this, there has to be a checkpoint in the MLPVAEMODULE config in order for the encoder to be loaded into the class
# model:
#   embedder: 'VAE'
#   embedder_ckpt: ${paths.log_dir}train_vae_cyclic/runs/2023-08-19_20-45-27_89023/checkpoints/epoch_045.ckpt

logger:
  wandb:
    tags: ${tags}
    group: "MultiTask MLP EFFICIENTNET VAE TEST"
  aim:
    experiment: "ECOACOUSTICS MultiTask MLP EFF VAE PREDICT"

umap_latent:
  seed: ${seed}
  model_name: 'VAE'
